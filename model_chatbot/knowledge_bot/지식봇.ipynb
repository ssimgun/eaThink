{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 모델 실행 == \n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./food_chatbot_model_최종\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./food_chatbot_model_최종\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # 입력 길이 제한 (512자까지 허용)\n",
    "    input_text = input_text[:512]\n",
    "    \n",
    "    # 입력 텍스트 인코딩\n",
    "    input_ids = tokenizer.encode(f\"질문: {input_text}\\n답변: \", return_tensors='pt').to(device)\n",
    "\n",
    "    # 답변 생성\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=200,  # 질문 길이 + 50자\n",
    "            min_length=20,  # 최소 답변 길이\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.2,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    # 출력 디코딩\n",
    "    output = tokenizer.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    # '답변:' 이후의 텍스트 추출\n",
    "    try:\n",
    "        response = output.split(\"답변:\")[1].strip()\n",
    "    except IndexError:\n",
    "        response = \"죄송해요, 답변을 생성하는 데 문제가 발생했어요.\"\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"음식 지식 챗봇입니다. 종료하려면 '종료'를 입력하세요.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"사용자: \")\n",
    "    if user_input.strip().lower() == \"종료\":\n",
    "        print(\"챗봇: 대화를 종료합니다. 좋은 하루 되세요!\")\n",
    "        break\n",
    "    response = generate_response(user_input)\n",
    "    print(f\"챗봇: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\F-2\\AppData\\Local\\Temp\\ipykernel_15748\\3839616795.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\ProgramData\\anaconda3\\envs\\ml-dev\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 중: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU 사용 중:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU를 사용하지 않고 있습니다. CPU 사용 중\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
